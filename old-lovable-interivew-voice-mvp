To understand how OpenAI receives data from voice in the interview sessions, I'll explain the full audio processing and transmission flow in the application.

The voice data flow in the interview session involves several steps:

Audio Capture: The browser captures audio via the microphone using WebRTC
Audio Processing: The application processes the raw audio into the correct format
Data Transmission: The processed audio is sent to OpenAI through a secure WebRTC connection
Response Handling: The application receives and processes audio/text responses from OpenAI
Let's look at each component of this flow:

1. Audio Recording and Processing
The application uses the AudioRecorder class in src/utils/audio/recorder.ts to capture microphone input. Here's how it works:

The recorder accesses the user's microphone using navigator.mediaDevices.getUserMedia
It creates an AudioContext with a specific sample rate (16kHz)
Audio processing happens through a ScriptProcessor node
The processed audio data (Float32Array) is passed to a callback function

// From AudioRecorder class in recorder.ts
processorNode.onaudioprocess = (e) => {
  const input = e.inputBuffer.getChannelData(0);
  this.processingCallback(input); // This passes the raw audio data to the callback
};
2. Audio Encoding for Transmission
Before sending to OpenAI, the raw audio needs conversion. This happens in the encodeAudioForAPI function in src/utils/audio/encoder.ts:

The Float32Array is converted to Int16Array (16-bit PCM format)
This is then base64 encoded for transmission
The encoded audio is properly formatted for OpenAI's Realtime API

// Audio encoding for API
const encodeAudioForAPI = (float32Array: Float32Array): string => {
  // Convert to Int16Array (required format for API)
  const int16Array = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    const s = Math.max(-1, Math.min(1, float32Array[i]));
    int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
  }
  
  // Convert to base64 string for transmission
  const uint8Array = new Uint8Array(int16Array.buffer);
  let binary = '';
  const chunkSize = 0x8000;
  
  for (let i = 0; i < uint8Array.length; i += chunkSize) {
    const chunk = uint8Array.subarray(i, Math.min(i + chunkSize, uint8Array.length));
    binary += String.fromCharCode.apply(null, Array.from(chunk));
  }
  
  return btoa(binary);
};
3. WebRTC Connection and Data Transmission
The useWebRTC hook manages the WebRTC connection to OpenAI. The flow involves:

Creating a secure connection via an Edge Function (realtime-session)
Getting an ephemeral API token through this function (to avoid exposing API keys)
Establishing a WebRTC peer connection with data channel
Sending the encoded audio through the data channel
From src/hooks/useWebRTC.ts:


// Inside startRecording function in useWebRTCAudio.ts
audioRecorderRef.current = new AudioRecorder((audioData) => {
  if (dataChannel.readyState === "open") {
    const encodedAudio = encodeAudioForAPI(audioData);
    dataChannel.send(JSON.stringify({
      type: "input_audio_buffer.append",  // This is the message type OpenAI expects
      audio: encodedAudio,  // This is the base64 encoded audio
    }));
  }
}, {
  recordLocally: true // Enable local recording
});
4. The OpenAI Realtime API Connection Process
The connection to OpenAI happens in several stages:

Session Creation: The app calls the realtime-session Edge Function
Token Generation: This function requests an ephemeral token from OpenAI
SDP Exchange: The WebRTC connection is established with Session Description Protocol (SDP) exchange
Client creates offer
Server returns answer
Connection established
From the realtime-session Edge Function:


// Request an ephemeral token from OpenAI
const response = await fetch("https://api.openai.com/v1/realtime/sessions", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${OPENAI_API_KEY}`,
    "Content-Type": "application/json",
  },
  body: JSON.stringify({
    model: "gpt-4o-realtime-preview-2024-12-17",
    // We'll send detailed instructions via session.update after connection
  }),
});
5. Message Format for Audio Transmission
When sending audio data to OpenAI, the application uses this specific format:


{
  type: "input_audio_buffer.append",
  audio: "base64EncodedAudioData"
}
The OpenAI Realtime API processes this audio data and:

Performs speech recognition
Processes the recognized speech through the chosen model
Generates responses as both text and synthesized speech
6. Voice Activity Detection (VAD)
The system uses server-side Voice Activity Detection:


// Configuration sent to OpenAI
{
  "turn_detection": {
    "type": "server_vad", 
    "threshold": 0.5,
    "prefix_padding_ms": 300,
    "silence_duration_ms": 800,
    "create_response": true
  }
}
This automatically detects when the user has stopped speaking and triggers responses from the AI.

7. Real-time Processing of Responses
The handleDataChannelMessage function in useWebRTCEvents.ts processes responses:

Audio data responses (response.audio.delta) are sent for playback
Transcript deltas (response.audio_transcript.delta) are accumulated
User speech transcriptions (input_audio_buffer.processed) are captured
Key Technical Details:
Audio Format: 16-bit PCM at 16-24kHz sample rate (mono)
Protocol: WebRTC with DataChannel for control messages
Encoding: Base64 for audio data transmission
Model: gpt-4o-realtime-preview-2024-12-17
This system allows for bidirectional audio communication between the user and OpenAI, creating a seamless conversational interview experience with real-time transcription.

---------------------------------------
Now that I understand the previous implementation, I can better explain the shift to the triangular architecture:
Previous Architecture (MVP)
Client-side audio processing (Float32Array → Int16Array → base64)
Direct WebRTC data channel to OpenAI
Edge function only for token/session setup
All processing happening on client device
New Triangular Architecture
Client captures raw audio
WebSocket transmission to Fly.io
Fly.io handles audio processing, encoding, and OpenAI communication
Supabase manages authentication and data persistence
The key differences:
Processing Location:
Before: Client device (inconsistent performance based on device)
Now: Fly.io servers (consistent, reliable processing)
Connection Protocol:
Before: WebRTC directly to OpenAI
Now: WebSockets to Fly.io, then HTTP to OpenAI
Security:
Before: Ephemeral tokens but still client-exposed
Now: API keys only on server side
With tokenized authentication, the flow would be:
Client validates token and establishes WebSocket with Fly.io
Fly.io handles all audio processing and OpenAI communication
Transcripts saved directly from Fly.io to Supabase
This maintains the real-time nature of the interviews while adding reliability and consistent performance, at the cost of a small increase in latency.



-----------
api.video Integration for Recording
Client-Side Recording:
Browser captures video stream alongside audio
Video stream sent to api.video in real-time
Each session gets a unique api.video recording ID
Parallel Processing:
While Fly.io handles audio → OpenAI → transcript
api.video simultaneously handles video recording
Both services reference the same session ID
Technical Implementation:
Generate api.video authentication token server-side
Client initializes recorder with this token
Video data never passes through Fly.io (separate stream)
Completed recording URL saved to interview session record
Post-Interview:
api.video provides processed video with playback URLs
System links transcript timestamps to video timeline
Creates complete interview package (video + transcript)
This approach means each service handles what it's best at:
Fly.io: Audio processing and AI communication
api.video: Video capture, encoding, storage, and delivery
Supabase: Session management and data persistence
The result is high-quality video recording without impacting the audio processing pipeline.